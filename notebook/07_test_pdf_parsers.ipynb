{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "import os \n",
    "\n",
    "DATA_DIR = '../data/'\n",
    "source = 'Disposizione Di Bonifico.pdf'  # document per local path or URL\n",
    "converter = DocumentConverter()\n",
    "result = converter.convert(os.path.join(DATA_DIR, source))\n",
    "print(result.document.export_to_markdown())  # output: \"## Docling Technical Report[...]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "\n",
    "class LineItem(BaseModel):\n",
    "    \"\"\"Una linea di un oggetto in una fattura.\"\"\"\n",
    "\n",
    "    item_name: str = Field(description=\"Il nome dell'articolo\")\n",
    "    price: float = Field(description=\"Il prezzo dell'articolo\")\n",
    "\n",
    "\n",
    "class Invoice(BaseModel):\n",
    "    \"\"\"Una rappresentazione di informazione di una fattura.\"\"\"\n",
    "\n",
    "    invoice_id: str = Field(\n",
    "        description=\"Un identificativo univoco per questa fattura. Spesso un codice alfanumerico.\"\n",
    "    )\n",
    "    date: datetime = Field(description=\"La data in cui la fattura è stata creata.\")\n",
    "    total_price: float = Field(description=\"Il prezzo totale della fattura.\")\n",
    "    iva: float = Field(description=\"L'IVA applicata alla fattura.\")\n",
    "    line_items: list[LineItem] = Field(\n",
    "        description=\"Una lista di tutti gli articoli compresi nella fattura\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Answer(BaseModel):\n",
    "    \"\"\"A generic structured answer to a question which requires dicotomic response (yes or no).\"\"\"\n",
    "    short_response : bool = Field(description=\"True if the answer to the question is yes, False otherwise\")\n",
    "    response: str = Field(description=\"The long and detailed answer to the question.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.program import FunctionCallingProgram\n",
    "\n",
    "# Initialize the Ollama LLM\n",
    "llm = Ollama(\n",
    "    model=\"phi4-mini\", \n",
    "    request_timeout=120.0,\n",
    "    format='json', \n",
    "    temperature=0.01,\n",
    "    num_ctx=32000\n",
    "    )\n",
    "\n",
    "# Define the prompt template\n",
    "prompt_template_str = (\n",
    "    \"Extract the invoice details from the following document:\\n\\n\"\n",
    "    \"{document}\\n\\n\"\n",
    ")\n",
    "\n",
    "# Create the LLMPydanticProgram\n",
    "#program = FunctionCallingProgram.from_defaults(\n",
    "#    output_cls=Invoice,\n",
    "#    prompt_template_str=prompt_template_str,\n",
    "#    llm=llm,\n",
    "#    verbose=True\n",
    "#)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#program(document=result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#source = 'E00324337228-IT4453739-z80K5gyUNjnHM3sO8hfJk0XP.pdf'  # document per local path or URL\n",
    "#converter = DocumentConverter()\n",
    "#result = converter.convert(source)\n",
    "#print(result.document.export_to_markdown())\n",
    "\n",
    "#program(document=result.document.export_to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Structured_Predict (using function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    \"Estrai i dati della fattura dal seguente testo:{text}\"\n",
    ")\n",
    "\n",
    "\n",
    "llm.structured_predict(\n",
    "    Invoice, \n",
    "    prompt=prompt, \n",
    "    text=result.document.export_to_markdown()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example combining RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "\n",
    "document = Document(text=result.document.export_to_markdown(), metadata={'source':os.path.join(DATA_DIR, source)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.node_parser import MarkdownNodeParser\n",
    "\n",
    "nodes = MarkdownNodeParser(include_metadata=True, include_prev_next_rel=True).get_nodes_from_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name='intfloat/multilingual-e5-small')\n",
    "Settings.embed_model = embed_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "index = VectorStoreIndex(nodes, embed_model=embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index.as_retriever(\n",
    "    similarity_top_k = 5,embed_model=embed_model\n",
    ").retrieve(\"Beneficiario fattura\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 5,\n",
    "    llm=llm.as_structured_llm(Invoice),\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"sberbank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get document and parse it to markdown\n",
    "\n",
    "# get markdown and build index\n",
    "\n",
    "# from index build query engine (with reranker if possible)\n",
    "\n",
    "# Option 1: Make question (aka checklist) <- envetually to be commbined with feature extraction features\n",
    "\n",
    "# Option 2: Combine with Pydantic and extract structured output (aka datafeed)\n",
    "\n",
    "# Option 3: Combine Retriever and Pydantic Template as \"tools\" for the workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k = 5,\n",
    "    llm=llm.as_structured_llm(Answer),\n",
    "    response_mode=\"tree_summarize\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine.query(\"Il conto è cointestato? Rispondi con sì o no.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Invoice.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Decomposing the Problem\n",
    "\n",
    "You need to:\n",
    "\n",
    "* ✅ Process text that exceeds the context window.\n",
    "* ✅ Extract structured information using a Pydantic schema.\n",
    "* ✅ Use RAG to retrieve relevant sections.\n",
    "* ✅ Utilize an agent to manage the flow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Suggested Approach\n",
    "Here’s a modular pipeline for handling the task:\n",
    "\n",
    "## A. Chunking & Indexing (Preprocessing Step)\n",
    "Break the long text into semantic chunks (e.g., using LangChain's RecursiveCharacterTextSplitter).\n",
    "Store these in a vector database (like FAISS, Chroma, Weaviate, etc.) for retrieval.\n",
    "## B. Agent as an Orchestrator\n",
    "The agent's role is to:\n",
    "* Interpret the query (e.g., \"Extract company details\" → determines which Pydantic model to use).\n",
    "* Retrieve relevant chunks from the vector store using RAG.\n",
    "* Pass the chunks to the LLM for structured extraction using the Pydantic class.\n",
    "* Aggregate results across multiple LLM calls (if necessary).\n",
    "## C. Query’s Role in the Process\n",
    "* The query defines what needs to be extracted from the long text.\n",
    "* It helps the agent filter and retrieve relevant chunks.\n",
    "Example queries:\n",
    "\"Extract all person names and their affiliations.\"\n",
    "\"Find product descriptions and their pricing information.\"\n",
    "\"Summarize legal clauses about termination conditions.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.llamaindex.ai/en/stable/module_guides/querying/structured_outputs/\n",
    "https://docs.llamaindex.ai/en/stable/understanding/extraction/lower_level/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
