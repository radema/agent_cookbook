{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define templates\n",
    "from datetime import datetime\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "\n",
    "class LineItem(BaseModel):\n",
    "    \"\"\"A line item in an invoice.\"\"\"\n",
    "    item_id: str = Field(description = \"Item identifier\")\n",
    "    item_name: str = Field(description=\"The name of this item\")\n",
    "    price: float = Field(description=\"The price of this item\")\n",
    "\n",
    "\n",
    "class Invoice(BaseModel):\n",
    "    \"\"\"A representation of information from an invoice.\"\"\"\n",
    "\n",
    "    invoice_id: str = Field(\n",
    "        description=\"A unique identifier for this invoice, often a number\"\n",
    "    )\n",
    "    date: datetime = Field(description=\"The date this invoice was created\")\n",
    "    line_items: list[LineItem] = Field(\n",
    "        description=\"A list of all the items in this invoice\"\n",
    "    )\n",
    "    quantity: float = Field(description=\"The bought quantity\")\n",
    "    vat:str = Field(description=\"The vat percentage applied, if applied. Otherwise, N/A.\")\n",
    "    gross_amount: float = Field(description=\"Invoice Gross amount.\")\n",
    "\n",
    "class BinaryAnswer(BaseModel):\n",
    "    \"\"\"A structured binary answer to a question.\"\"\"\n",
    "\n",
    "    answer: bool = Field(description=\"The answer to the question. True for yes, False for no. None otherwise.\")\n",
    "    details: Optional[str] = Field(description=\"Explanation related to the answer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tools\n",
    "from llama_index.core.program.function_program import get_function_tool\n",
    "\n",
    "invoice_tool = get_function_tool(Invoice)\n",
    "binary_answer_tool = get_function_tool(BinaryAnswer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define agent\n",
    "from llama_index.core.agent.workflow import AgentWorkflow, FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define llm\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "phi_llm = Ollama(\n",
    "    model=\"phi4-mini:latest\",\n",
    "    base_url=\"http://localhost:11434\", \n",
    "    request_timeout=360.0,\n",
    "    temperature=0.01\n",
    "    )\n",
    "\n",
    "minion_llm = Ollama(\n",
    "    model=\"gemma3:4b\",\n",
    "    base_url=\"http://localhost:11434\", \n",
    "    request_timeout=360.0,\n",
    "    temperature=0.01\n",
    "    )\n",
    "\n",
    "#response = llm.complete(\"What is the capital of France?\")\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create specialized agents\n",
    "extract_agent = FunctionAgent(\n",
    "    name=\"ExtractAgent\",\n",
    "    llm=phi_llm,\n",
    "    description=\"Extract structured information from a provided based on given tools. Use always tool.\",\n",
    "    system_prompt=\"You are a meticoulous document reviewer able to extract specific information...\",\n",
    "    tools=[invoice_tool],\n",
    "    can_handoff_to=[\"AnswerAgent\"]\n",
    ")\n",
    "\n",
    "answer_agent = FunctionAgent(\n",
    "    name = \"AnswerAgent\",\n",
    "    llm=phi_llm,\n",
    "    description=\"Provide the final answer with the most suitable structure based on available tools. Use always tool.\",\n",
    "    system_prompt=\"You are precise answerer...\",\n",
    "    tools=[binary_answer_tool]\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "agent_workflow = AgentWorkflow(\n",
    "    agents=[extract_agent, answer_agent],\n",
    "    \n",
    "    root_agent=\"ExtractAgent\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent_workflow.run(user_msg=\"What is the weather in San Francisco?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_files_gen = Path(\"./data/\").rglob(\"*\")\n",
    "all_files = [f.resolve() for f in all_files_gen]\n",
    "all_pdf_files = [f for f in all_files if f.suffix.lower() == \".pdf\"]\n",
    "len(all_pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Document\n",
    "from docling.document_converter import DocumentConverter\n",
    "\n",
    "converter = DocumentConverter()\n",
    "\n",
    "doc_limit = 100\n",
    "\n",
    "docs = []\n",
    "\n",
    "loaded_docs = converter.convert_all(all_pdf_files[:])\n",
    "\n",
    "text=converter.convert(all_pdf_files[-1]).document.export_to_markdown()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"The gross amount is higher than 60 euros?This is the document:{text}. Provide a structured answer.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await agent_workflow.run(user_msg=prompt)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = agent_workflow.run(user_msg=prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import (\n",
    "    AgentInput,\n",
    "    AgentOutput,\n",
    "    ToolCall,\n",
    "    ToolCallResult,\n",
    "    AgentStream,\n",
    ")\n",
    "\n",
    "current_agent = None\n",
    "current_tool_calls = \"\"\n",
    "async for event in handler.stream_events():\n",
    "    if (\n",
    "        hasattr(event, \"current_agent_name\")\n",
    "        and event.current_agent_name != current_agent\n",
    "    ):\n",
    "        current_agent = event.current_agent_name\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"ü§ñ Agent: {current_agent}\")\n",
    "        print(f\"{'='*50}\\n\")\n",
    "\n",
    "    # if isinstance(event, AgentStream):\n",
    "    #     if event.delta:\n",
    "    #         print(event.delta, end=\"\", flush=True)\n",
    "    # elif isinstance(event, AgentInput):\n",
    "    #     print(\"üì• Input:\", event.input)\n",
    "    elif isinstance(event, AgentOutput):\n",
    "        if event.response.content:\n",
    "            print(\"üì§ Output:\", event.response.content)\n",
    "        if event.tool_calls:\n",
    "            print(\n",
    "                \"üõ†Ô∏è  Planning to use tools:\",\n",
    "                [call.tool_name for call in event.tool_calls],\n",
    "            )\n",
    "    elif isinstance(event, ToolCallResult):\n",
    "        print(f\"üîß Tool Result ({event.tool_name}):\")\n",
    "        print(f\"  Arguments: {event.tool_kwargs}\")\n",
    "        print(f\"  Output: {event.tool_output}\")\n",
    "    elif isinstance(event, ToolCall):\n",
    "        print(f\"üî® Calling Tool: {event.tool_name}\")\n",
    "        print(f\"  With arguments: {event.tool_kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sllm = llm.as_structured_llm(Invoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = sllm.complete(prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_info = handler.text\n",
    "from llama_index.core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "answer_prompt = PromptTemplate('Given an answer to the following question based on the given context.\\nQuestion:{question}\\nContext:{context}')\n",
    "llm.structured_predict(BinaryAnswer, answer_prompt, question=\"L'importo totale √® minore di 50 euro?\", context=structured_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
